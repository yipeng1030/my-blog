## 一、从进程说起

容器，在其最核心的层面，就是一个标准的Linux进程。因此理解容器先从Linux内核如何看待和管理最基本的执行单元——进程——开始。

### 1.1 进程描述符，理解tast_struct

一个磁盘上的二进制文件一旦被执行，就变成了由计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件以及各种设备的状态信息组成的一个集合。而内核对一个进程的全部认知都封装在tast_struct的C语言结构体中。这个结构体是关于进程所有信息的“唯一真实来源”。

#### task_struct的核心结构与演进

task_struct的关键字段包括

- state：描述进程的当前状态（运行、休眠等）
- pid：进程ID
- Parent / children：指向父进程以及子进程列表的指针，构成了系统的进程树
- Files：指向files_struct的指针，记录了进程打开的文件描述符
- Fs: 指向fs_struct的指针，描述文件系统相关状态
- Mm：指向mm_struct的指针，描述了进程的内存地址空间
- nsproxy：指向nsproxy的指针，包含了与该进程关联的所有命名空间的信息。

为了让运行中的内核代码能够快速找到当前正在执行进程的task_struct，内核提供了一个与具体架构相关的宏——current。在x86架构上，current通过对当前栈指针进行位掩码操作，高效地定位到栈底的thread_info结构，然后通过其task指针找到完整的task_struct 

进程的生命周期由其state字段定义，主要包括下面几种状态

- TASK_RUNNING：进程可运行，它要么正在CPU上执行，要么在运行队列中等待执行。  
- TASK_INTERRUPTIBLE：可中断的睡眠状态。进程正在等待某个条件（如I/O完成），可以被信号唤醒。  
- TASK_UNINTERRUPTIBLE：不可中断的睡眠状态。与前者类似，但不会被信号唤醒，通常用于必须等待且不能被干扰的短时操作，例如等待磁盘I/O。  
- __TASK_STOPPED：进程已停止，例如通过SIGSTOP信号。  
- __TASK_TRACED：进程被另一个进程（如调试器）通过ptrace跟踪。

内核通过parent和children指针维护着整个系统的进程家族树，所有进程最终都可追溯到PID为1的init进程。同时，所有的task_struct实例都被组织在一个称为task_list的**循环双向链表**中，内核可以通过for_each_process等宏来遍历所有进程 。

### 1.2 fork()到clone()：创建一个进程

在Linux中，clone()系统调用是创建新执行上下文的真正原语。传统的fork()以及用于创建线程的pthread_create()，现在都只是Glibc库对clone()系统调用使用不同标志组合的封装。

- fork()：源自经典的UNIX模型，它创建一个与父进程几乎完全相同的子进程，拥有独立的地址空间 。早期的  fork()因需要完整复制父进程内存而效率低下。后来引入的**写时复制（Copy-on-Write, COW）**技术极大地优化了这一点。COW机制下，父子进程初始时共享同一份物理内存页，只有当其中一个进程尝试写入时，内核才会为该进程复制一份新的内存页。这使得fork()的开销主要集中在复制页表和为子进程创建唯一的task_struct上 。  
- clone()：Linux特有的功能强大的系统调用。接受一系列标志位参数，用“掩码+或”的方式精确控制子进程与父进程之间共享或复制哪些资源 。这些标志包括：  
  - CLONE_VM：共享内存空间（地址空间）。  
  - CLONE_FILES：共享文件描述符表。  
  - CLONE_FS：共享文件系统信息（如root目录、umask）。  
  - **CLONE_NEW系列标志：创建新的命名空间（如CLONE_NEWPID, CLONE_NEWNET），这是容器隔离的核心。**

当一个程序调用fork()时，Glibc的包装函数实际上会调用clone()并传入一组预设的标志，以模拟传统fork()的行为。而当调用pthread_create()时，Glibc则会调用clone()并传入共享资源的标志（如CLONE_VM），从而创建一个线程。

内核中，clone()的实现主要由kernel/fork.c中的do_fork()函数处理。该函数会调用copy_process()，后者负责完成大部分工作，包括：调用dup_task_struct()为子进程分配新的内核栈、thread_info和task_struct；检查资源限制；根据clone()的标志来决定是复制还是共享文件、文件系统信息、信号处理器、地址空间和命名空间等。传统的进程是完全隔离的，而传统的线程几乎是完全共享的。容器需要一个中间地带：隔离的PID、网络和挂载点，但可能共享用户ID或其他资源。

#### 内核线程与用户线程：统一的视角

Linux内核对线程的实现独具一格：它没有为线程定义一个独立于进程的数据结构。在内核看来，**每个线程都是一个标准的进程**，只是它选择与创建它的进程共享某些资源，如地址空间（mm_struct）和文件描述符表（files_struct。每个线程都有自己唯一的task_struct和PID，并由内核独立调度。

此外，还有一种特殊的线程——**内核线程**。它们是只在内核空间运行的标准进程，没有用户地址空间（其task_struct中的mm指针为NULL）。内核线程负责执行后台任务，如刷新磁盘缓存、处理内核工作队列等

## 二、namespace实现隔离

namespace提供了隔离性。

### 2.1 命名空间原理：所见即全部

命名空间可以改变进程的视野，使其看到独立的资源实例。例如，PID命名空间内的进程拥有自己独立的PID编号，而mnt命名空间内的进程拥有自己独立的文件系统挂载点视图。

- **基本特性**

  - 每个进程都属于每种类型的一个命名空间（mnt, pid, net, ipc, uts, user, cgroup, time）

  - 通过clone()或unshare()系统调用并附带相应的**CLONE_NEW**标志来创建新的命名空间

  - 命名空间在/proc/<pid>/ns/目录下被具象化为伪文件。这些文件可以作为句柄，用于setns()系统调用，让一个进程加入一个已存在的命名空间

    - 这个setns()系统调用也是`docker exec -it /bin/bash`的原理，内核把一个/bin/bash进程加入到容器的进程组里，从而进入容器的视野。

  - 当命名空间中的最后一个进程退出后，该命名空间通常会被销毁，除非其对应的/proc文件被持久化地bind 挂载

    - Bind Mount是linux的一种独特的挂载方式，允许内核将一个目录或文件直接挂载到另一个目录中，其原理实际上是一个inode替换的过程。在linux中，可以把inode理解为存放文件内容的“对象”，而dentry（目录项）就是访问这个inode所使用的“指针”，而一次bind mount其实相当于将“挂载点的dentry”重定向到了“被挂载的目录或文件的inode”。

      - | 比较点        | Bind mount                                     | 硬链接                                   | 符号链接                                         |
        | ------------- | ---------------------------------------------- | ---------------------------------------- | ------------------------------------------------ |
        | 作用域        | 挂载点级别（可以跨文件系统）                   | 仅同一个文件系统                         | 人一路径                                         |
        | 原始路径删除  | 不影响bind挂载，挂载点仍然存在，仍指向原 inode | 链接计数-1，若计数归零，文件就真的被删除 | 直接失效，留下一个指向不存在路径的“悬空”快捷方式 |
        | 是否新增inode | 否（共享原 inode）                             | 否（共享原 inode）                       | 是（新建 inode）                                 |
        | 指向对象      | 目录、文件都行（目录还能递归）                 | 只能指向文件（目录禁止硬链接）           | 目录、文件、甚至不存在的路径都行                 |

### 2.2 挂载（mnt）命名空间

mnt命名空间为进程提供了一份私有的文件系统挂载点列表。这是为容器提供独立于宿主机的根文件系统的基础 。

- **工作机制**：  
  - 创建一个新的挂载命名空间的时候，子进程会获得父进程挂载表的一份副本。此后，在新的命名空间内执行的mount和umount操作不会影响到父命名空间。
  - 由于mount的隔离必须发生在新进程创建之后，所以如果一个容器想要拥有一个干净的文件系统视野，就必须进行一次根目录的重新挂载，这里有chroot和pivot_root两种方式。
- Chroot vs pivot_root:
  - chroot是一个较早的机制、简单的机制，它仅改变进程的根目录，但并非一个真正的隔离，一个特权进程有多种办法可以从其中逃逸。
    - chroot到底改了啥？系统调用：`chroot("/newroot")`
      - 仅把当前进程的 task->fs->root（VFS 根 dentry）换成 `/newroot`。
      - 没创建新的挂载命名空间，因此： – 所有老挂载点仍挂在全局挂载树上； – `/proc/mounts` 里仍然列着宿主机全部挂载； – 挂载传播关系（shared/sub mounts）完全保留。
      - 没隔离挂载权限：拥有 `CAP_SYS_ADMIN` 的进程在 chroot 里照样能 `mount`/`umount`
    - 所以直接把当前工作目录切到 chroot 外父目录，然后再次 chroot 回真实根就逃逸了
  - pivot_root()是容器运行时所使用的更为健壮的系统调用。它会改变整个挂载命名空间的根挂载点。它将旧的根目录挂载到新根目录下的一个指定位置，然后将新目录提升为根目录，因为旧根已被转移且没有进程以它为根，于是可以卸载旧挂载点让宿主侧挂载彻底消失，实现干净切换。
  - 使用pivot_root需要满足一些严格条件，例如new_root必须是一个挂载点，且不能与当前根在同一个挂载点上，为了满足这个条件，容器运行时通常会使用一个技巧
    - mount --bind new_root new_root，将新根目录绑定挂载到自身，从而使其成为一个挂载点

### 2.3 PID命名空间：私有进程树与PID1的角色

PID命名空间提供了隔离的进程ID空间。不同PID命名空间中的进程可以拥有相同的PID。

- **核心行为**：  
  - 在一个新的PID命名空间中创建的第一个进程，其PID被赋予为**1**，并成为该命名空间的**init进程**。  
  - 这个PID为1的进程承担着特殊职责：它会收养其命名空间内所有的孤儿进程（即父进程先于子进程退出的进程）。  
  - **关键影响**：如果PID为1的进程终止，内核会向该PID命名空间内的所有其他进程发送SIGKILL信号，从而终止整个容器。此后，向该命名空间fork()新进程的尝试会失败 。  
- /proc文件系统：仅仅创建一个新的PID命名空间是不够的。默认情况下，新空间内的进程看到的仍然是宿主机的/proc文件系统，这会导致ps等命令显示出宿主机的所有进程，造成视图混乱 。
  - 解决方案是同时创建一个新的挂载命名空间，并在其中挂载一个全新的、只感知当前PID命名空间的/proc文件系统。unshare命令的--mount-proc选项就是为此设计的，它自动化了这个过程 。
  - 命名空间的隔离不仅仅是隔离数据本身（如进程树），还必须隔离用于观察这些数据的接口（如/proc文件系统）。有效的容器化需要将多个命名空间组合起来，才能创造出一个一致且可信的虚拟环境。

### 2.4 网络（net）命名空间：构建独立的网络栈

网络命名空间为进程提供了一套完全独立的网络协议栈，包括：网络接口（含一个私有的lo环回设备）、IP地址、路由表和iptables防火墙规则 。

- **从零到一的连接**：  
  - 一个新创建的网络命名空间是完全隔离的，初始时只有一个处于DOWN状态的lo设备，无法与外界通信。  
  - 连接性是通过**veth pair**（虚拟以太网设备对）建立的。veth pair就像一根虚拟网线，有两端。  
  - 一端被移入容器的网络命名空间另一端保留在宿主机的根命名空间中。  
  - 在宿主机上，通常会创建一个**Linux bridge**（虚拟交换机），并将所有容器在宿主机端的veth接口都连接到这个网桥上。这使得同一宿主机上的多个容器可以相互通信。  
  - 为了让容器能够访问外部网络（如互联网），需要在宿主机上启用IP转发，并设置一条iptables的NAT规则，将来自容器私有网段的流量的源IP地址伪装成宿主机的公共IP地址再发送出去。

### 2.5 用户（user）命名空间：无根容器与权限作用域

用户命名空间是所有命名空间中对安全影响最为深远的一个。它隔离了用户ID（UID）和组ID（GID），使得一个进程可以在命名空间内部拥有root权限（UID为0），而在命名空间外部却是一个普通的非特权用户。

- **安全模型的变革**：  
  - 它通过/proc/<pid>/uid_map和/proc/<pid>/gid_map文件来定义内部UID/GID与宿主机UID/GID之间的映射关系。例如，可以将容器内的UID 0映射到宿主机的UID 1000。 
  - **安全意义**：如果一个在用户命名空间中以root身份运行的进程设法“逃逸”出容器，它在宿主机上只拥有一个普通用户的权限，能造成的破坏被大大限制。  
  - 创建一个新的用户命名空间会赋予进程在该命名空间内**一套完整的权能（Capabilities）**。这使得进程可以在其自己的其他命名空间内执行管理任务（如配置网络接口、挂载文件系统），而无需在宿主机上拥有CAP_SYS_ADMIN这样的高权限。

用户命名空间从根本上改变了Linux的安全模型，一个进程可以拥有与root相关的**能力**，但这些能力被限制在一个有限的**作用域**（即其命名空间）内，而不再是全局的、无所不能的宿主机root。这是一种从传统的二元（root/non-root）模型向更精细、更安全的权限模型的范式转移

### 2.6 辅助命名空间：UTS、IPC、Cgroup

- **UTS (UNIX Time-sharing System)**：隔离hostname和NIS域名。允许每个容器拥有自己独立的主机名。  
- **IPC (Inter-Process Communication)**：隔离System V IPC对象（如消息队列、信号量、共享内存）和POSIX消息队列。防止不同容器间的进程通过这些IPC机制相互干扰。  
- **Cgroup**：隔离cgroup文件系统视图。当容器内的进程读取/proc/self/cgroup时，它看到的是相对于其所在cgroup的路径，而不是宿主机上的完整路径。  

| 命名空间      | CLONE_* 标志    | unshare 选项 | 隔离的资源                             | 在容器化中的目的                               |
| ------------- | --------------- | ------------ | -------------------------------------- | ---------------------------------------------- |
| Mount (mnt)   | CLONE_NEWNS     | -m, --mount  | 文件系统挂载点列表                     | 提供独立的根文件系统和/proc视图                |
| PID           | CLONE_NEWPID    | -p, --pid    | 进程ID号空间                           | 提供独立的进程树，实现容器内PID 1              |
| Network (net) | CLONE_NEWNET    | -n, --net    | 网络设备、协议栈、端口、路由表、防火墙 | 提供独立的网络环境，避免端口冲突               |
| User          | CLONE_NEWUSER   | -U, --user   | 用户ID和组ID、权能                     | 实现无根容器（Rootless Container），提升安全性 |
| IPC           | CLONE_NEWIPC    | -i, --ipc    | System V IPC、POSIX消息队列            | 隔离进程间通信资源                             |
| UTS           | CLONE_NEWUTS    | -u, --uts    | 主机名和NIS域名                        | 允许每个容器有自己的主机名                     |
| Cgroup        | CLONE_NEWCGROUP | -C, --cgroup | Cgroup根目录视图                       | 隐藏宿主机cgroup结构，增强隔离感               |
| Time          | CLONE_NEWTIME   | -T, --time   | 单调时钟和启动时钟                     | 支持检查点/恢复和时间相关的测试                |

### 2.7 实践操作：unshare与nsenter指南

unshare和nsenter是两个用户空间工具，它们分别对应unshare()和setns()系统调用，可以用来进行命名空间实验和调试

- unshare：允许一个正在运行的进程改变其执行上下文，特别是它所处的namespace，而无需创建新进程。一个常见的模式是，父进程先调用unshare()创建一组新的命名空间，然后调用fork()。这样新创建的进程会直接继承这些全新的、隔离的命名空间，从而在一个沙箱化的环境中运行。
- nsenter：进入一个已存在进程的命名空间。  

## 三、cgroups实现限制

命名空间解决了“隔离”问题，而控制组（Cgroups）则解决了“限制”问题。它确保一个被隔离的进程不能无限制地消耗系统资源，从而影响其他进程或宿主机本身。

### 3.1 Cgroup的层级结构：

Cgroups将进程组织成层级化的分组，并对这些组应用资源限制 。

- **Cgroups v1**：允许存在多个并行的层级结构。例如，cpu控制器和memory控制器可以分别挂载在不同的层级上。这意味着一个进程可以同时属于/sys/fs/cgroup/cpu/groupA和/sys/fs/cgroup/memory/groupB。这种模型非常灵活，但也极其混乱和难以理解，因为不同资源限制之间的关系变得不明确。  

cgroups语境下的控制器：对于某种资源，负责具体实施对一组进程（也即一个Cgroup）的对应资源的限制、审计和控制。

- **Cgroups v2**：2 通过“统一层级 + 无内部进程规则”把 v1 时代“多挂载、多树、进程到处跑”的混乱局面，变成了 一棵总树、一条路径、一个叶子节点承载进程 的简洁模型。所有启用的控制器都挂载在同一个树状结构下（通常是/sys/fs/cgroup）。一个进程在任何时候都只属于这个树中的一个节点，这使得资源策略的管理变得清晰和一致。控制器在每个子树上通过向  cgroup.subtree_control文件写入控制器名称来启用或禁用。  
- **“无内部进程”规则（v2）**：这是v2的一个核心设计原则。进程只能被放置在cgroup树的**叶子节点**上。一个cgroup如果拥有子cgroup（即它是一个用于分配资源的“中间节点”），那么它自身就不能包含任何进程。清晰地区分了用于组织进程的组和用于控制资源分配的组。

从v1到v2的转变使得cgroup变得更可预测、更易于治理。统一层级和“无内部进程”规则避免了控制器之间复杂且常常出人意料的相互作用，使得系统行为更容易被推断。在v1中，管理员可以在一个层级中设置cpu.shares，在另一个层级中设置blkio.weight，这两个独立限制之间的关系是不明确的。而在v2中，cpu和io控制器都在同一个层级中，一个进程只属于一个组，这迫使管理员必须制定一个协调一致的资源策略，从而实现更确定性的系统级资源分配。

### 3.2 CPU控制器与完全公平调度器（CFS）

CPU控制器负责管理CPU时间的分配。它与内核的完全公平调度器（Completely Fair Scheduler, CFS）协同工作，提供两种主要的控制方式：按比例共享（相对权重）和硬性限制（带宽控制）。

- **CFS基础**：CFS是Linux的默认调度器，其目标是为每个任务提供公平的CPU时间。它通过一个名为“虚拟运行时间”（vruntime）的计数器来追踪每个任务已经运行了多长时间。调度器总是选择vruntime最小的任务来运行，从而在宏观上保证公平性 。  
- **相对份额（Shares）**：  
  - **v1**: cpu.shares。默认值为1024。一个shares为2048的组在CPU资源紧张时，能获得的CPU时间大约是一个shares为1024的组的两倍。这是一种相对的、弹性的限制 。  
  - **v2**: cpu.weight。功能类似，但范围是1-10000，默认值为100。  
- **硬性限制（Bandwidth Control）**：  
  - **v1**: cpu.cfs_period_us 和 cpu.cfs_quota_us。这两个参数共同定义了一个带宽限制。在一个period（周期，单位微秒）内，一个cgroup中的所有进程总共可以使用的CPU时间不能超过quota（配额，单位微秒）。例如，设置period为100000 (100ms)，quota为50000 (50ms)，就意味着该cgroup最多只能使用一个CPU核心的50% 。  
  - **v2**: cpu.max。提供了一个更简洁的接口。例如，echo "50000 100000"可以达到与上述v1配置相同的效果

### 3.3 Cpuset控制器：将负载绑定到CPU和内存节点

Cpuset控制器可以将一个cgroup内的进程**固定**在指定的CPU核心集合和内存节点（NUMA node）集合上运行。这对于需要高性能、低延迟的应用至关重要，可以避免因跨NUMA节点访问内存而带来的性能损失。

- **核心配置**：  
  - cpuset.cpus：指定允许使用的CPU核心列表，格式可以是逗号分隔的列表或连字符表示的范围（如0-3,7）
  - cpuset.mems：指定允许使用的内存节点列表（如0,1）
  - **强制参数**：在使用cpuset子系统时，必须为cgroup定义cpuset.cpus和cpuset.mems的值，然后才能将任务移入该组 
  - **独占设置**：通过cpu_exclusive和mem_exclusive标志，可以为某个cgroup独占CPU和内存节点，防止其他cgroup使用这些资源

### 3.4 内存控制器：控制内存使用与OOM Killer

内存控制器用于跟踪和限制cgroup的内存使用量（包括物理内存和交换空间）

- **硬性限制**：  
  - **v1**: memory.limit_in_bytes。如果一个cgroup的内存使用量超过这个值，其内部的进程就会成为**OOM（Out Of Memory）Killer**的目标。  
  - **v2**: memory.max。功能等同于v1的硬限制。当达到此限制时，内核会调用OOM Killer来终止该cgroup中的进程 。  
- **v2的精细化控制：保护与节流**：Cgroups v2引入了更为精细的内存控制模型。它从简单的“限制并杀死”模型演变为“保护、节流、然后杀死”的模型。  
  - memory.high：这是一个节流（throttle）限制。当内存使用超过这个阈值时，该cgroup中的进程在申请新内存时会被强制进行内存回收（节流），执行速度会显著变慢。这个过程发生在OOM之前，提供了一种“软”失败模式。  
  - memory.low：这是一个尽力而为（best-effort）的保护。如果一个cgroup的内存使用量低于这个“低水位线”，内核会尽量避免回收它的内存。  
  - memory.min：这是一个**硬性保证**。内核会为了满足这个cgroup的最小内存需求而去杀死其他未受保护的进程。

这种新模型允许系统更优雅地处理内存压力。memory.high的节流行为是一个可观测的事件（通过memory.events文件），监控系统可以捕捉到这个信号，从而在应用程序被OOM杀死**之前**就向运维人员发出警报。这为调试内存泄漏或进行扩容等干预措施创造了一个宝贵的时间窗口，实现了从被动响应（OOM）到主动管理（节流）的转变。

- **Cgroup感知的OOM Killer**：当内存限制被触发时，cgroup感知的OOM Killer会选择在**违规的cgroup内部**杀死进程，而不是在系统范围内随机选择一个进程。这对于多租户环境的稳定性至关重要 。

## 四、分层文件系统提高镜像分发效率

解释完容器间的墙，但如果容器进程低头一看会是怎样呢？本节将解释由多个只读层构成的容器镜像是如何高效地存储，并以一个统一、可写的文件系统形式呈现给容器的。

### 4.1 联合文件系统（Union Filesystems）的概念

overlaylfs是文件系统之上的文件系统。可以称之为上层文件系统。它能够将多个目录（称为“层”）的内容叠加在一起，并呈现为一个单一、合并后目录树的技术 。这是实现容器镜像分层和写时复制（CoW）策略的基础。它允许多个容器共享一个只读的基础镜像，同时每个容器都在其上拥有一个自己专属的、轻量的可写层，用于存放所有修改。

### 4.2 OverlayFS详解：lowerdir, upperdir, merged与workdir

OverlayFS是现代Linux内核内置的联合文件系统，已成为容器运行时的标准选择。它将一个可写的upperdir叠加在一个或多个只读的lowerdir之上。

- **核心目录**：  
  - lowerdir：只读的底层。在Docker中就是镜像的各个层。  
    - Docker build制作镜像，就是通过每个关键字为基础镜像分别添加一个增量的只读层。
  - upperdir：可写的顶层。在Docker中，这就是容器专属的可写层，所有修改都会保存在这个目录中。
  - merged：呈现给用户或容器的统一视图，也就是容器的根目录挂载点。  
  - workdir：一个内部使用的空目录，OverlayFS需要它来完成原子操作。它必须与upperdir位于同一个文件系统上。  
- 写时复制（Copy-Up）操作：  
  - 当一个进程试图修改一个只存在于lowerdir中的文件时，OverlayFS会首先将这个文件从lowerdir复制到upperdir。之后的所有修改都将作用于upperdir中的这个新副本，而lowerdir中的原始文件保持不变 。如果要删除一个  lowerdir中的文件，OverlayFS会在upperdir中创建一个特殊的“白障（whiteout）”文件来标记该文件已被删除 。

### 4.3 containerd的实现

1. 镜像（lowerdir）
   1. 在containerd中，用户pull拉镜像时，会返回多个层，这些层都是压缩文件。`/var/lib/containerd/io.containerd.content.v1.content/blobs/sha256/`
   2. containerd会把这些压缩文件解压然后存到`/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/<id>/fs`中。每拉取一层就解压成一份只读快照，目录名是递增数字编号
   3. 字节的机器上把官方默认的 `/var/lib/containerd/ `改到了` /opt/tce/run/`
2. 可写层（upperdir + workdir）
   1. 当通过 ctr / crictl / Kubelet 创建容器时，containerd 会在镜像最顶层之上再新建一个快照目录 `/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/<new-id>/`其中 `fs/` 是 upperdir，`work/` 是 OverlayFS 的 workdir。这一层就是容器的“可写层”，所有运行时写入都发生在这里
3. 运行时挂载的 rootfs（merged）
   1. 启动时通过mount命令创建一个 overlay文件系统。这样用户在容器的更新的文件都存在upper层了
   2.  `Mount -t overlay overlay -o lowerdir=/low1,low2,low3,upperdir=/upper,workdir=/work`
   3. containerd 把 lowerdir 列表 + upperdir 以 OverlayFS 的形式联合挂载到 `/run/containerd/io.containerd.runtime.v2.task/<namespace>/<container-id>/rootfs` 这个目录就是容器进程看到的“/”，即 rootfs。

## 五、加固容器——安全机制

本节将探讨超越命名空间隔离和cgroup限制的额外安全层，它们用于进一步加固容器，防范内核级别的攻击。

### 5.1 Linux权能（Capabilities）：将root权力拆分

传统的UNIX模型中，root用户（UID 0）拥有至高无上的权力。Linux权能（Capabilities）机制将root用户的庞大特权分解为近40个独立的、细粒度的单元，如CAP_NET_BIND_SERVICE（允许绑定到1024以下的端口）或CAP_SYS_CHROOT（允许使用chroot）。

- **最小权限原则**：容器运行时（如Docker）默认会为容器启动一个**受限的权能集合**，丢弃了大量高风险的权能，如CAP_SYS_ADMIN（允许执行大量系统管理操作）和CAP_SYS_MODULE（允许加载内核模块）。这遵循了安全领域的  **最小权限原则**。  
- **权能集**：每个线程都维护着多个权能集，其中最主要的是effective（当前生效的）、permitted（可以被提升为生效的）和inheritable（可以被子进程继承的）。  
- **工具**：getpcaps命令可以查看进程的权能，而setcap命令可以将权能赋予一个可执行文件，从而替代了高风险的setuid位。
- **举例：**比如容器进程没有CAP_SYS_RESOURCE 权限，不能绕过配额子系统。https://docs.kernel.org/filesystems/quota.html

### 5.2 Seccomp-BPF：系统调用过滤

Seccomp（Secure Computing Mode）与BPF（Berkeley Packet Filter）的结合，允许一个进程定义一个过滤器，严格限制它自身可以调用的系统调用（syscall）。这是在应用程序与内核之间建立的安全边界。

- **工作原理**：  
  - Seccomp配置文件是一个JSON文件，它定义了一个**默认动作**（如SCMP_ACT_ERRNO，表示拒绝调用并返回权限错误），然后提供一个允许调用的**系统调用白名单** 。  
  - Docker为所有容器默认启用一个seccomp配置文件，该文件阻止了大约44个最危险或最不常用的系统调用，如reboot, kexec_load, add_key等 。这个默认配置已经成功地防御了一些现实世界中的容器逃逸漏洞 。  
  - 用户可以通过--security-opt seccomp=<profile.json>选项为容器指定一个自定义的、更严格的配置文件，以实现更强的安全性 。  
  - 社区也开发了一些工具，如oci-seccomp-bpf-hook，它可以通过eBPF追踪一个应用程序在正常运行期间实际使用的系统调用，从而自动生成一个最小化的、遵循最小权限原则的seccomp配置文件。

命名空间和cgroups提供了资源隔离和限制，但容器进程仍然通过系统调用与共享的宿主机内核交互。如果某个被允许的系统调用存在漏洞，攻击者就可能利用它从容器内部攻击宿主机。权能和seccomp提供了关键的第二道防线：权能限制了进程可以尝试的**特权操作**的种类，而seccomp则更进一步，直接过滤进程可以发起的**系统调用**本身。即使一个权能被授予，如果其关联的系统调用对应用程序来说并非必需，seccomp也可以将其阻止。这些机制协同工作：命名空间进行**隔离**，cgroups进行**限制**，权能进行**特权收缩**，seccomp进行**API过滤**。任何一层的防御失效，都可能被另一层捕获。

## 六、待研究——tce现在的容器有什么优化